{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9743e88e",
   "metadata": {},
   "source": [
    "**Тема курсовой работы**: Система для сбора и генерации постов в социальных сетях\n",
    "\n",
    "**Цель**: Построить модель машинного обучения, которая наиболее точно способна формировать текст, основываясь на начальных данных из обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc72f6",
   "metadata": {},
   "source": [
    "**Задачи**:\n",
    "\n",
    "1. Провести предобработку и изучение данных.\n",
    "\n",
    "2. Подготовить выборку.\n",
    "\n",
    "3. Построить и обучить модель.\n",
    "\n",
    "4. Cделать выводы о способностях модели генерировать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f7c71",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \\\n",
    "После проведенной работы модель способна генерировать сложные предложения, исходя из существующей совокупности предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f6aa6",
   "metadata": {},
   "source": [
    "**Входные данные**:\n",
    "\n",
    "Исходный dataset содержит выдержку текстов Льва Толстого на русском языке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfcef2",
   "metadata": {},
   "source": [
    "**Практическая часть:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c347e0",
   "metadata": {},
   "source": [
    "Корпус, на котором тренируется модель- выборка из Льва Толстого (текстовый файл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680666d",
   "metadata": {},
   "source": [
    "Из данного текста выделяем необходимую последовательность слов (функция get_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f1745",
   "metadata": {},
   "source": [
    "Получившийся генератор tokens будет выдавать «очищенную» последовательность слов и знаков препинания. Однако, нам интересны тройки токенов (слово или знак препинания, т.е. некие атомарные элементы текста). \\\n",
    "Для этого добавляем еще один генератор, на выходе которого будем иметь три подряд идущих токена (gen_trigrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342243a",
   "metadata": {},
   "source": [
    "gen_trigrams - cимволы '$' используются для маркировки начала предложения. В дальнейшем, это делает проще выбор первого слова генерируемой фразы. \\\n",
    "В целом, метод действует следующим образом: он возвращает три подряд идущих токена, на каждой итерации сдвигаясь на один токен: см. Пример "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f446e",
   "metadata": {},
   "source": [
    "Пример:\n",
    "\n",
    "Вход:\\\n",
    "'Порок есть наказание без удовлетворения'\\\n",
    "Выход:\\\n",
    "0: $ $ порок\\\n",
    "1: $ порок есть\\\n",
    "2: порок есть наказание\\\n",
    "3: есть наказание без\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8a55a",
   "metadata": {},
   "source": [
    "Далее, рассчитаем триграммную модель (train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1ce39",
   "metadata": {},
   "source": [
    "В первой части этого метода задаем генераторы. Далее, рассчитываем биграммы и триграммы (фактически, считаем количество одинаковых пар и троек слов в тексте). Далее, вычисляем вероятность слова в зависимости от двух предыдущих, помещая данное слово и его вероятность в словарь. \\\n",
    "Примечание: Это не самый оптимальный метод, так как идет значительный расход памяти. Но для небольших наборов данных этого вполне достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb51d7",
   "metadata": {},
   "source": [
    "Теперь модель готова для генерации текста. Возвращаем предложение (generate_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bf6e3",
   "metadata": {},
   "source": [
    "Суть метода в последовательной выборке наиболее вероятных слов или знаков препинания до тех пор, пока не встретится признак начала следующей фразы (символа $). Первое слово выбирается как наиболее вероятное для начала предложения из набора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13012a",
   "metadata": {},
   "source": [
    "Словарь model для каждой пары слов содержит список пар (слово, вероятность). Нам же необходимо выбрать из этого набора только одно слово. Если выбрать слово с максимальной вероятностью, тогда все сгенерированные фразы были бы похожи друг на друга. \\\n",
    "Более подходящий способ способ — выбирать слова с некой хаотичностью, которая бы зависела от вероятности слова (так как нам не нужно, чтобы наши фразы состояли из редких сочетаний). Это и делает метод unirand, который возвращает случайное слово с вероятностью, равной вероятности данного слова в зависимости от двух предыдущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706d02de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Файл содержит 3422485 непустых символа\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Админ\\Downloads\\tolstoy.txt', encoding='utf-8') as f:\n",
    "    num_chars = len(re.sub('\\s', '', f.read()))\n",
    "    print (f' Файл содержит {num_chars} непустых символа')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104886d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Посылается генерал для осмотра позиции.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "from random import uniform\n",
    "from collections import defaultdict\n",
    "\n",
    "r_alphabet = re.compile(u'[а-яА-Я0-9-]+|[.,:;?!]+')\n",
    "\n",
    "def gen_lines(corpus):\n",
    "    data = open(corpus, encoding='utf-8')\n",
    "    for line in data:\n",
    "        yield line.lower()\n",
    "\n",
    "def gen_tokens(lines):\n",
    "    for line in lines:\n",
    "        for token in r_alphabet.findall(line):\n",
    "            yield token\n",
    "\n",
    "def gen_trigrams(tokens):\n",
    "    t0, t1 = '$', '$'\n",
    "    for t2 in tokens:\n",
    "        yield t0, t1, t2\n",
    "        if t2 in '.!?':\n",
    "            yield t1, t2, '$'\n",
    "            yield t2, '$','$'\n",
    "            t0, t1 = '$', '$'\n",
    "        else:\n",
    "            t0, t1 = t1, t2\n",
    "\n",
    "def train(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "\n",
    "    bi, tri = defaultdict(lambda: 0.0), defaultdict(lambda: 0.0)\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        bi[t0, t1] += 1\n",
    "        tri[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}\n",
    "    for (t0, t1, t2), freq in tri.items():\n",
    "        if (t0, t1) in model:\n",
    "            model[t0, t1].append((t2, freq/bi[t0, t1]))\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, freq/bi[t0, t1])]\n",
    "    return model\n",
    "\n",
    "def generate_sentence(model):\n",
    "    phrase = ''\n",
    "    t0, t1 = '$', '$'\n",
    "    while 1:\n",
    "        t0, t1 = t1, unirand(model[t0, t1])\n",
    "        if t1 == '$': break\n",
    "        if t1 in ('.!?,;:') or t0 == '$':\n",
    "            phrase += t1\n",
    "        else:\n",
    "            phrase += ' ' + t1\n",
    "    return phrase.capitalize()\n",
    "\n",
    "def unirand(seq):\n",
    "    sum_, freq_ = 0, 0\n",
    "    for item, freq in seq:\n",
    "        sum_ += freq\n",
    "    rnd = uniform(0, sum_)\n",
    "    for token, freq in seq:\n",
    "        freq_ += freq\n",
    "        if rnd < freq_:\n",
    "            return token\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = train(r'C:\\Users\\Админ\\Downloads\\tolstoy.txt')\n",
    "    print(generate_sentence(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819068c2",
   "metadata": {},
   "source": [
    "Дополнение:\\\n",
    "Триграммная модель выбрана, так как биграммы дают плохой результат, в то время как 4-граммы требуют существенно больше ресурсов.\\\n",
    "Расширить алгоритм для случая N-грамм возможно, но при увеличении числа N, текст будет все больше походить на исходный"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfa421",
   "metadata": {},
   "source": [
    "**Вывод**: Наша модель успешно формирует предложения с сохранением правил русского языка и наличием смысла внутри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb99c1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
